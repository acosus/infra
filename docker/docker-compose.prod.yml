version: "3.8"

services:
  frontend:
    # build:
    # context: ../frontend
    # dockerfile: Dockerfile
    image: ${DOCKER_USERNAME}/frontend:${GITHUB_SHA}
    env_file:
      - ../frontend/.env
    ports:
      - "80:80"
      - "443:443"
    environment:
      - NODE_ENV=production
      # NO NEED TO SET VITE_API_URL HERE BECAUSE NGINX WILL HANDLE THE PROXY
      - VITE_API_URL=http://backend:3000
    restart: unless-stopped
    depends_on:
      - backend
      - ml-model
    networks:
      - frontend
      - backend
      - ml
    # volumes:
    # - ./ssl:/etc/ssl:ro
    logging:
      driver: "json-file"
      options:
        max-size: "200m"
        max-file: "10"

  backend:
    # build:
    # context: ../backend
    # dockerfile: Dockerfile
    image: ${DOCKER_USERNAME}/backend:${GITHUB_SHA}
    expose:
      - "3000"
    environment:
      - NODE_ENV=production
      - ML_ROOT_URL=http://ml-model:5051
      - CORS_ORIGIN=${CORS_ORIGIN}
    env_file:
      - ../backend/.env
    restart: unless-stopped
    networks:
      - backend
      - ml
    logging:
      driver: "json-file"
      options:
        max-size: "200m"
        max-file: "10"

  ml-model:
    # build:
    # context: ../model
    # dockerfile: Dockerfile
    image: ${DOCKER_USERNAME}/model:${GITHUB_SHA}
    expose:
      - "5051"
    environment:
      - MODELENV=production
      - FLASK_HOST=0.0.0.0
      - FLASK_PORT=5051
      - EXPRESS_URL=http://backend:3000
      - USE_HARDCODED_DATA=False
      - FLASK_DEBUG=False
    env_file:
      - ../model/.env
    restart: unless-stopped
    volumes:
      - ml_models:/app/models
    networks:
      - ml
      - backend
    logging:
      driver: "json-file"
      options:
        max-size: "200m"
        max-file: "10"

networks:
  frontend:
  backend:
  ml:

volumes:
  ml_models:
    driver: local
